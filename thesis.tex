\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
%\usepackage{lstlistings}

\title{Clustering big data}
\author{Dan Filimon}

\begin{document}
\maketitle

\tableofcontents

\pagebreak

\section{Rationale}

Telescopes surveying the Universe in search of exoplanets, DNA sequencers
decoding our genes, governments monitoring cities, and people all over the
world exchanging messages, images, videos produce across the Internet produce
exabytes of data every day.

Researchers, businesses and governments all over the world are trying to make
sense of the ever-increasing collection of data they have access to.

It needs to be stored, searched through, processed and analyzed to extract
valuable information.
Such a large volume of data comes with its own set of challenges and in the
past few years, novel programming paradigms have emerged that make analyzing
this data a lot more accessible.
There are many systems that are designed to handle ``big data''. Out of these, the most famous one is MapReduce \cite{mr1} from Google, for batch
processing and more recently Storm \cite{st1} originally from Backtype, now acquired by
Twitter.

Many applications that have previously been impossible are now a reality thanks
to massive data sets. For example, quality statistical machine Translation can only
work at very large scale. While the accuracy of currently available commerical
fails to capture more subtle nuances, it is certainly good enough to convey the
general meaning.

Large scale data analysis is a broad field with many overlapping disciplines
like machine learning, data mining, data science etc.

Analyzing big data is challening. The complexity of many algorithms makes them
difficult to parallelize and even when this is possible, because of the scale
of the data, clusters of many machines are needed to process the data in
reasonable time.

The mainframe era of the 80s, displaced by the affordable
personal computer is making a comeback under the guise of ``cloud computing''
precisely because the complexity of maintaining a large enough cluster of machines
to process ``big data'' makes it a ripe target for outsourcing.

Companies now offer ``clouds'', clusters of virtual machines that support
MapReduce style processing through open-source Hadoop distributions like Amazon
(Amazon Web Services), Google (Google Compute Engine), Microsoft (Windows
Azure) have vast data centers at their disposal and offer infrastructure as a
service to be billed by the hour (or minute).

Time is quite literally money in this market which makes fast processing even
more critical. When processing data at terrabye scale and beyond especially in
pipelines, the first priority is usually to reduce the data through some
aggregation such that the resulting size is much smaller, for example storing
statistics instead of the raw data.

One of the most useful algorithms to analyze data with is clustering.
Intuitively the aim of clustering is to find clumps of data that are more
similar to one another than the others.

The traditional use of clustering is for revealing interesting patterns in the
data --- for example, if $n$ good quality clusters do form, this could mean there
are $n$ types of users in the system.

It's also especially useful for getting a good representative sample of the data
that should behave similarly to the original distribution.
This is important because sometimes there are no obvious ways of aggregating the
data being processed directly to reduce its size. If the data is clustered
however, the resulting clusters' centroids can be used as input for the next
stage of processing.

\section{Clustering}

There are many kinds of clustering which vary significantly in their approach
in building the clusters. The kind of clustering we'll be discussing in this
paper is ``centroid'' based.

To apply any clustering, or machine learning for that matter, the data needs to
be transformed from its raw representation to ``feature vectors''. The
corresponding feature vector for a data item should capture plausibly relevant
characteristics of the item in a numeric form.
For example, when working with text documents, feature vectors containing
frequencies a given word in a document compared to the corpus are useful.

This paper doesn't attempt to further describe the various ways used to encode
data into feature vectors or assess the usefulness of a subset of features.
This is requires domain-specific knowledge and is the goal of feature engineering.
We will assume the data is already available as feature vectors and will now
formalize the clustering problem.

We are given $n$ vectors (also referred to as a points) in $\textbf{R}^d$ and an
integer $k$, and a distance measure, $dist$. We aim to group the $n$ points
into $k$ disjoint sets $X_1$ through $X_k$. The mean of
the points in cluster $i$ (or, disjoint set $X_i$) is called the ``centroid''
of that cluster, which we call $c_i$.

We want our clustering to be good in some sense, so we must define a measure of
quality to optimize for. For this, we need a distance measure, $dist$, making the combined vector space and measure a metric space.

We use $T_c$, the total cost of the clustering, which is the sum of the distances
from each point to the centroid of the cluter it is assigned to.

\begin{eqnarray}
    T_c = \sum_{i = 1}^{k} \left( \sum_{\bf x_{ij} \in X_i} dist({\bf x_{ij}},
    {\bf c_i})
    \right)
\end{eqnarray}

In this formulation, the number of clusters, $k$ is fixed, but this problem is
related to the facility placement problem, which essentially identical except
in that the number of cluster can vary.

The main issue with this formulation is that the optimization problem we're
trying to solve is NP-hard in the general case.
This has the very important implication that it is infeasible to find an
optimal solution to this problem. Any polynomial-time algorithm we can devise
will at best be an approximation scheme.
This turns out to not be as dire as it first sounds, because ``good enough'' is
fine for virtually all applications and also because real data tends to have
additional properties that results in stronger quality guarantees.

\subsubsection{Distance measure}
One difference users used to clustering literature might have noticed in this
formulation is that our
choice of distance measure is not fixed. Normally most papers on clustering
work with $||{\bf x - y}||_2^2$, the squared Euclidean distance (or squared
$L2$-norm). We acknowledge this and indeed some results we use for our
implementations have this assumption. As implementors of a machine learning
library however, we need to support user extensibility and we feel that other
than providing documentation to our users, we shouldn't enforce other kinds of
restrictions. It turns out that most distances used in practice
are variants of this distance measure (for example the $L2$-norm and the cosine
distance), in which case the results likely the same. If some completely
different distance measure is used, we can't provide any guarantees.

\subsection{Quality}

When it comes to the quality of the clusters generated by any algorithm, it is
often said that ``clustering is in the eye of the beholder''.
In unsupervised learning, which this problem is a part of, there is no
known ``right answer''. Compare this with classification or regression where an
example is either correctly clasified or predicted, or it isn't.

Things are not so clear-cut in this case. While the total cost is certainly
what we optimize for, multiple measure have been devised over the years that
attempt to formalize the degree a clustering is ``good''.
Intuitively, we want clusters that are:
\begin{itemize}
    \item compact, meaning that the points in a cluster are close together (the
        intra-cluster distances are small between any two points)
    \item well separated, meaning that two different clusters will be
        relatively far apart (the inter-cluster distances are large between any
        two clusters)
\end{itemize}

The Dunn Index and Davies-Bouldin Index try to express compactness and
separability in one score. These are called ``internal'' scores because they
only at one clustering.

Additionally, it is often useful to compare two different clusterings and to
see how similar they are. This is useful especially when comparing different
clusteirng algorithms. A widely used score for this is the Adjusted Rand Index
based off the ``confusion matrix'' (same idea as with classification).

\subsubsection{Dunn Index}
The Dunn Index, was invented in 1974 by J. Dunn. A higher Dunn Index indicates
a better clustering. It combines $\Delta_i$, a distance score for cluster $i$
(called intracluster distance) and the distance between two clusters,
$dist(\mathbf{c}_i, \mathbf{c}_j)$ (intercluster distance).

$\Delta_i$ can have different expressions. For cluster $X_i$ it could be:
\begin{itemize}
    \item the maximum distance between any two points
        \begin{eqnarray}
            \Delta_i = \max_{\mathbf{x}, \mathbf{y} \in X_i} dist(\mathbf{x},
            \mathbf{y}) \;\textrm{}
        \end{eqnarray}
    \item the mean distance between any two points
        \begin{eqnarray}
            \Delta_i = \frac{1}{|X_i| (|X_i| - 1)} \sum_{\mathbf{x}, \mathbf{y} \in
            X_i, \mathbf{x} \ne \mathbf{y}} dist(\mathbf{x}, \mathbf{y})
        \end{eqnarray}
    \item the mean distance between any point and the centroid
        \begin{eqnarray}
            \Delta_i = \frac{\sum_{\mathbf{x} \in X_i} dist(\mathbf{x},
            \mathbf{c}_i)}{|X_i|}
        \end{eqnarray}
    \item the median distance between any point and the centroid. This is the
        one we use in the implementation. The reasoning is that the median is
        much more roubst to outliers than the mean or max. Additionally,
        computing distances between all pairs of points is simply not feasible
        for large datasets.
\end{itemize}

Having defined $\Delta_i$, the Dunn Index is:

\begin{eqnarray}
    D = \min_{1 \le i \le k} \left\{ \min_{1 \le j \le k, j \ne i} \left\{
        \frac{dist(\mathbf{c}_i, \mathbf{c}_j)}{\max_{1 \le l \le k} \Delta_l}
    \right\} \right\}
\end{eqnarray}

\subsubsection{Davies-Bouldin Index}
The Davies-Bouldin Index was invented in 1979 by D. Davies and D. Bouldin. Like
the Dunn Index, it is an internal evaluation scheme. A lower Davies-Bouldin
Index indicates a better clustering.

Defining a cluster-specific measure, exactly like for the Dunn Index,
$\Delta_i$, the index is:
\begin{eqnarray}
    DB = \frac{1}{k} \sum_{i = 1}^k \max_{j \ne i} \left( \frac{\Delta_i +
    \Delta_j}{dist(\mathbf{c}_i, \mathbf{c}_j)} \right)
\end{eqnarray}

\subsubsection{Adjusted Rand Index}
The Rand Index was invented by W. Rand in 1971. It measures how similar two
clusterings are to one another. When the index is adjusted for change grouping
of elements it is known as the Adjusted Rand Index.

To compute the index, one must first construct a contingency table (also known
as a confusion matrix). Assuming the clusterings are $X = \{X_1, X_2, \ldots
X_k\}$ and $Y = \{Y_1, Y_2, \ldots Y_k\}$, the overlap between cluster $i$ of
$X$, and cluster $j$ of $Y$, $n_{ij}$ is the number of points that are closest
to cluster $i$ in clustering $X$ and cluster $j$ in clustering $Y$. That is
$n_{ij} = X_i \cap Y_j$.

\begin{center}
    \begin{tabular}{c | c  c  c  c | c}
        & $Y_1$ & $Y_2$ & $\ldots$ & $Y_k$ & Sums \\
        \hline
        $X_1$ & $n_{11}$ & $n_{12}$ & $\ldots$ & $n_{1k}$ & $a_1$ \\
        $X_2$ & $n_{21}$ & $n_{22}$ & $\ldots$ & $n_{2k}$ & $a_2$ \\
        $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$ \\
        $X_k$ & $n_{k1}$ & $n_{k2}$ & $\ldots$ & $n_{kk}$ & $a_k$ \\
        \hline
        Sums & $b_1$ & $b_2$ & $\ldots$ & $b_k$ & \\
    \end{tabular}
\end{center}

The Adjusted Rand Index is:

\begin{eqnarray}
    AR = \frac{\sum_{ij} \left( \frac{n_{ij}}{2} \right) - \left[\sum_i
        \left( \frac{a_i}{2} \right) \sum_j \left( \frac{b_j}{2} \right)
        \right]/\left( \frac{n}{2} \right)}{\frac{1}{2} \left[ \sum_i \left(
            \frac{a_i}{2} \right) + \sum_j \left( \frac{b_j}{2} \right) \right] - \left[\sum_i
        \left( \frac{a_i}{2} \right) \sum_j \left( \frac{b_j}{2} \right)
        \right]/\left( \frac{n}{2} \right)}
\end{eqnarray}

\section{k-means}

The most famous, simple and quite venerable clustering algorithm, known since the 50s
is k-means and later Lloyd's method.

It first starts by chosing $k$ point out of the $n$ as seeds. These will be the
first centroids (they're not ``really'' centroids since they are not the means
of the points).
The algorithm then proceeds by doing some number of iterations of the following
steps:
\begin{enumerate}
    \item assign each point to the cluster whose centroid is closest to it
    \item recompute the existing centroids
\end{enumerate}
Multiple iterations are performed until the maximum number of iterations is
reached, until the quality plateaus or until no point changes cluster during
the assignment phase.

\begin{figure}[H]
    \caption{These are some example points we'll cluster in 3 groups}
    \includegraphics[scale=0.4]{kmeans-points-pre.pdf}
\end{figure}

\begin{figure}[H]
    \caption{Here we selected 3 points as the initial centroids}
    \includegraphics[scale=0.4]{kmeans-points-init.pdf}
\end{figure}

\begin{figure}[H]
    \caption{Here we assign each point to the cluster whose centroid it's
    closest to}
    \includegraphics[scale=0.4]{kmeans-points-assign1.pdf}
\end{figure}

\begin{figure}[H]
    \caption{After asigning each point to a cluster, we compute the centroids
    of those clusters as the mean of the points in that cluster}
    \includegraphics[scale=0.4]{kmeans-points-adjust1.pdf}
\end{figure}

\begin{figure}[H]
    \caption{Second iteration assignment}
    \includegraphics[scale=0.4]{kmeans-points-assign2.pdf}
\end{figure}

\begin{figure}[H]
    \caption{Second iteration centroid adjustment}
    \includegraphics[scale=0.4]{kmeans-points-adjust2.pdf}
\end{figure}

\pagebreak

\subsection{Algorithm}

The Python-style pseudocode for the algorithm described above is:
\begin{verbatim}
centroids = select k points from n
while not done:
    for p in points:
        cmin = None
        for c in centroids:
            d = dist(p, c)
            if d < dmin:
                dmin = d
                cmin = c
\end{verbatim}

The crucial (unexplained) details are:
\begin{enumerate}
    \item centroid initialization: how are the $k$ points selected to become
        centroids at the beginning?
    \item stopping condition: how long do we need to do the point assignment
        and centroid adjustment?
\end{enumerate}
These two questions have a variety of solutions each of them resulting in a
slightly different version of the algorithm, but with mostly similary results.

\subsection{Initialization}
Selecting the first $k$ points to become centroids, or ``seeding'' as it is
also known is the single most important factor in getting a high-quality
clustering.

\subsubsection{Random selection}
The simplest solution is to simply select $k$ points of the $n$ randomly. This
produces a seed but has no real guarantees about what these points are.
For a good quality clustering, assuming there are $k$ real clusters and we wish
to identify them, the seeds should ideally come from the $k$ clusters and no
two seeds should come from the same clusters.
If this is true, because we assumed the clusters really do exist in the data,
the cluster assignment will set nearly all the points in the right cluster in
just a few steps.
The downside of this approach is illustrated in the figure below. There, two
seeds are selected from the same real cluster, causing it to be split among the
two k-means clusters (red and yellow). This means, another cluster (the blue
one) will have to contain points from two real clusters.
\begin{figure}[H]
    \caption{Here is a case where the random initialization failed and a
    single real cluster was split between the yellow and red k-means clusters}
    \includegraphics[scale=0.7]{kmfail.png}
\end{figure}
Additionally, like many machine learning algorithms that only result in locally
optimal solutions, k-means benefits from multiple restarts. Multiple restarts
are even more important as this makes it nearly impossible to not have a good
set of initial centroids.

\subsubsection{k-means++}
There are ways of improving the selection of centroids through the intuitive
idea that we want to have seeds that are as far apart to each other as
possible. This effectively eliminates the problem of having two seeds in the
same cluster. While there are multiple ways of doing this, the most widely used
one probably being \cite{Arthur2007}. The approach I implemented is described
in \cite{Ostrovsky} and is summarized below. In \cite{Ostrovsky}, Ostrovsky et.
all introduce a new condition, called {\it $\epsilon$-separability} that
formalizes what data should look like for a good clustering to exist and give
an new way of sampling the initial centroids with provably-good guarantees.
This sampling is fairly similar to k-means++ and they show how it can produce a
constant-factor approximation of the optimal clustering with just one Lloyd
step provided the data is $\epsilon$-separable.

From an implementor's perspective, checking if the data is indeed
$\epsilon$-separable is of little interest. We would need to know the optimum
clustering cost for $k$ clusters and $k - 1$ clusters and if we could compute
those, we'd just do it directly.
The new sampling method however, is useful and we describe it below (based
on section 4.1.1 in \cite {Ostrovsky}).

First, we select two centroids, $c_1$ and $c_2$ with probability proportional
to $dist(c_1, c_2)$. \cite{Ostrovsky} uses $dist({\bf x}, {\bf y}) = ||{\bf x -
y}||_2^2$ as the distance measure, but as explained above, we generalize this
in the implemetation to any user supplied function. We now have the first 2
centroids.

Then, to select the $(i + 1)$-th centroid, call the $i$ already selected centers,
$\mathbf{c}_1 \ldots \mathbf{c}_i$. The new point is selected randomly with
probability proportional to $\min_{j \in \left\{ 1 \ldots i \right\}}
dist(\mathbf{x}, \mathbf{c}_j)$.

\subsubsection{Convergence}

As with most iterative machine learning algorithms, the main k-means step is
performed multiple times. We stop the algorithm after:
\begin{itemize}
    \item a fixed number of iterations, this is usually an upper bound
    \item a quality metric plateaus (in this case, total cost), meaning that
        it stops decreasing after a few iterations
    \item no points change cluster assignment
\end{itemize}

\subsubsection{Outliers}

\subsubsection{Nearest neighbor search}

\subsection{Ball k-means}

\section{Large scale}

\section{Batch processing: MapReduce}

\section{Online processing: Storm}

\section{k-means as a MapReduce}

\subsection{Algorithm}

\subsection{Discussion}

\subsubsection{Multiple passes through the data}

\subsubsection{Random initialization}

\section{Making big data small}

\section{Streaming k-means}

\subsection{Algorithm}

\subsection{Discussion}

\section{Clustering big data: MapReduce}

\subsection{Mahout implementation}

\section{Clustering big data: Storm}

\subsection{Storm prototype}

\section{Results}

\subsection{Quality}

\subsubsection{Data sets}

\subsubsection{Quality measures}

\subsubsection{Experiment}

\subsubsection{Discussion}

\subsection{Speed}

\subsubsection{Data sets}

\subsubsection{Cluster}

\subsubsection{Experiment}

\subsubsection{Discussion}

\section{Conclusion}


%\begin{thebibliography}
    %\bibitem Arthur, D., & Vassilvitskii, S. (2007). k-means++ : The Advantages of
        %Careful Seeding. Proceedings of the eighteenth annual ACM- …, 8, 1–11.
        %Retrieved from http://dl.acm.org/citation.cfm?id=1283383.1283494
%\end{thebibliography}
\end{document}
